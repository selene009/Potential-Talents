{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-09-06T07:28:45.882439Z","iopub.execute_input":"2022-09-06T07:28:45.882939Z","iopub.status.idle":"2022-09-06T07:28:45.898737Z","shell.execute_reply.started":"2022-09-06T07:28:45.882897Z","shell.execute_reply":"2022-09-06T07:28:45.897349Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('../input/potential-talents/potential-talents - Aspiring human resources - seeking human resources.csv')","metadata":{"execution":{"iopub.status.busy":"2022-09-06T07:28:50.027383Z","iopub.execute_input":"2022-09-06T07:28:50.028699Z","iopub.status.idle":"2022-09-06T07:28:50.045075Z","shell.execute_reply.started":"2022-09-06T07:28:50.028616Z","shell.execute_reply":"2022-09-06T07:28:50.043929Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"data['input'] = \"Aspiring human resources seeking human resources\"","metadata":{"execution":{"iopub.status.busy":"2022-09-06T07:28:50.428124Z","iopub.execute_input":"2022-09-06T07:28:50.429184Z","iopub.status.idle":"2022-09-06T07:28:50.440250Z","shell.execute_reply.started":"2022-09-06T07:28:50.429124Z","shell.execute_reply":"2022-09-06T07:28:50.438771Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"from nltk import word_tokenize, pos_tag\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\nimport re \nimport string \ndef removestopwords(text):\n    stop_words = set(stopwords.words('english'))\n#     print(stop_words)\n    text = re.sub(r'[^\\w\\s]', \" \", text)\n    text  = [word for word in text.split() if word not in stop_words]\n    \n    return text    \n\ndef lemmatization(text):\n    words = removestopwords(text)\n#     print(words)\n#     words = word_tokenize(words)\n#     word_tagged = pos_tag(words)\n#     print( word_tagged)\n    lemmatizer = WordNetLemmatizer()\n    text = \" \".join([lemmatizer.lemmatize(word) for word in words])\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-09-06T07:28:50.853547Z","iopub.execute_input":"2022-09-06T07:28:50.855183Z","iopub.status.idle":"2022-09-06T07:28:50.865522Z","shell.execute_reply.started":"2022-09-06T07:28:50.855121Z","shell.execute_reply":"2022-09-06T07:28:50.864547Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"import nltk\nnltk.download('omw-1.4')","metadata":{"execution":{"iopub.status.busy":"2022-09-06T07:28:51.269777Z","iopub.execute_input":"2022-09-06T07:28:51.270543Z","iopub.status.idle":"2022-09-06T07:28:51.761146Z","shell.execute_reply.started":"2022-09-06T07:28:51.270500Z","shell.execute_reply":"2022-09-06T07:28:51.760092Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"data['job_title'] = data['job_title'].apply(lambda x: lemmatization(x))\ndata['job_title'] ","metadata":{"execution":{"iopub.status.busy":"2022-09-06T07:28:52.182801Z","iopub.execute_input":"2022-09-06T07:28:52.184066Z","iopub.status.idle":"2022-09-06T07:28:52.226585Z","shell.execute_reply.started":"2022-09-06T07:28:52.184019Z","shell.execute_reply":"2022-09-06T07:28:52.225640Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"# **word2vec**","metadata":{}},{"cell_type":"code","source":"import nltk\nimport gensim\nfrom gensim.models import Word2Vec\ntokens = data['job_title'].apply(lambda x: nltk.word_tokenize(x))\nw2v_model = Word2Vec(tokens,\n                     min_count=1,\n                     window=10,\n                     vector_size=250,\n                     alpha=0.03, \n                     min_alpha=0.0007,\n                     workers = 4,\n                     seed = 42)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T07:28:53.365501Z","iopub.execute_input":"2022-09-06T07:28:53.366012Z","iopub.status.idle":"2022-09-06T07:28:53.418136Z","shell.execute_reply.started":"2022-09-06T07:28:53.365968Z","shell.execute_reply":"2022-09-06T07:28:53.416900Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"from scipy import spatial\n    \nindextokey_set = set(w2v_model.wv.index_to_key)\ndef avg_feature_vector(sentence, w2v_model, num_features, index2word_set):\n    words = sentence.split()\n    feature_vec = np.zeros((num_features, ), dtype='float32')\n    n_words = 0\n    for word in words:\n            if word in index2word_set:\n                n_words += 1\n                feature_vec = np.add(feature_vec, w2v_model.wv[word])\n                \n            if (n_words > 0):\n                feature_vec = np.divide(feature_vec, n_words)\n    return feature_vec  ","metadata":{"execution":{"iopub.status.busy":"2022-09-06T07:28:53.791001Z","iopub.execute_input":"2022-09-06T07:28:53.791439Z","iopub.status.idle":"2022-09-06T07:28:53.800054Z","shell.execute_reply.started":"2022-09-06T07:28:53.791404Z","shell.execute_reply":"2022-09-06T07:28:53.798781Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"score1 = []\nfor sentence in data['job_title']:\n    s1_afv = avg_feature_vector(sentence, w2v_model, num_features=250, index2word_set=indextokey_set)     \n    score1.append(s1_afv)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T07:28:54.241524Z","iopub.execute_input":"2022-09-06T07:28:54.242624Z","iopub.status.idle":"2022-09-06T07:28:54.254498Z","shell.execute_reply.started":"2022-09-06T07:28:54.242558Z","shell.execute_reply":"2022-09-06T07:28:54.253109Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"score2 = []\nfor sentence in data['input']:\n    s2_afv = [avg_feature_vector(sentence, w2v_model, num_features=250, index2word_set=indextokey_set)]\n    score2.append(s2_afv)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T07:28:54.640273Z","iopub.execute_input":"2022-09-06T07:28:54.641330Z","iopub.status.idle":"2022-09-06T07:28:54.650181Z","shell.execute_reply.started":"2022-09-06T07:28:54.641278Z","shell.execute_reply":"2022-09-06T07:28:54.648953Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"# the output is Cosine simlilarity with word2vec word embedding \nsim_word2vec = []\nfor i in range(len(score1)):\n    sim = 1 - spatial.distance.cosine(score1[i],score2[i])\n    sim_word2vec.append(sim)\nprint(sim_word2vec)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T07:28:55.158570Z","iopub.execute_input":"2022-09-06T07:28:55.159769Z","iopub.status.idle":"2022-09-06T07:28:55.176074Z","shell.execute_reply.started":"2022-09-06T07:28:55.159724Z","shell.execute_reply":"2022-09-06T07:28:55.174637Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"markdown","source":"# **TFIDF**","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\ndef cosine_sim(text1, text2):\n    tfidf = vectorizer.fit_transform([text1, text2])\n    return ((tfidf * tfidf.T).A)[0,1]","metadata":{"execution":{"iopub.status.busy":"2022-09-06T07:28:56.261641Z","iopub.execute_input":"2022-09-06T07:28:56.264360Z","iopub.status.idle":"2022-09-06T07:28:56.277982Z","shell.execute_reply.started":"2022-09-06T07:28:56.264293Z","shell.execute_reply":"2022-09-06T07:28:56.274825Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"# the output is Cosine simlilarity with tfidf vectorizer \nsim_tfidf = []\nfor i in range(len(data['job_title'])):\n    score = cosine_sim(data['job_title'][i], data['input'][i])\n    sim_tfidf.append(score)\nprint(sim_tfidf)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T07:28:56.967761Z","iopub.execute_input":"2022-09-06T07:28:56.968923Z","iopub.status.idle":"2022-09-06T07:28:57.162147Z","shell.execute_reply.started":"2022-09-06T07:28:56.968870Z","shell.execute_reply":"2022-09-06T07:28:57.160768Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Glove**","metadata":{}},{"cell_type":"code","source":"# !wget http://nlp.stanford.edu/data/glove.6B.zip\n# !unzip glove.6B.zip","metadata":{"execution":{"iopub.status.busy":"2022-09-06T07:29:00.182740Z","iopub.execute_input":"2022-09-06T07:29:00.184181Z","iopub.status.idle":"2022-09-06T07:29:00.189161Z","shell.execute_reply.started":"2022-09-06T07:29:00.184128Z","shell.execute_reply":"2022-09-06T07:29:00.187841Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"GLOVE_EMB = '/kaggle/working/glove.6B.300d.txt'\nEMBEDDING_DIM = 300","metadata":{"execution":{"iopub.status.busy":"2022-09-06T07:29:01.459469Z","iopub.execute_input":"2022-09-06T07:29:01.461132Z","iopub.status.idle":"2022-09-06T07:29:01.466832Z","shell.execute_reply.started":"2022-09-06T07:29:01.461079Z","shell.execute_reply":"2022-09-06T07:29:01.464989Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(data['job_title'])\n\nword_index = tokenizer.word_index\nvocab_size = len(tokenizer.word_index) + 1\nprint(\"Vocabulary Size :\", vocab_size)\nword_index.keys()","metadata":{"execution":{"iopub.status.busy":"2022-09-06T07:29:02.290725Z","iopub.execute_input":"2022-09-06T07:29:02.292133Z","iopub.status.idle":"2022-09-06T07:29:02.305387Z","shell.execute_reply.started":"2022-09-06T07:29:02.292083Z","shell.execute_reply":"2022-09-06T07:29:02.304008Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"embeddings_index = {}\n\nf = open(GLOVE_EMB)\nfor line in f:\n  values = line.split()\n  word = value = values[0]\n  coefs = np.asarray(values[1:], dtype='float32')\n  embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' %len(embeddings_index))","metadata":{"execution":{"iopub.status.busy":"2022-09-06T07:29:02.414906Z","iopub.execute_input":"2022-09-06T07:29:02.415850Z","iopub.status.idle":"2022-09-06T07:29:28.064591Z","shell.execute_reply.started":"2022-09-06T07:29:02.415792Z","shell.execute_reply":"2022-09-06T07:29:28.063215Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","metadata":{"execution":{"iopub.status.busy":"2022-09-06T07:29:28.067738Z","iopub.execute_input":"2022-09-06T07:29:28.068302Z","iopub.status.idle":"2022-09-06T07:29:28.076490Z","shell.execute_reply.started":"2022-09-06T07:29:28.068247Z","shell.execute_reply":"2022-09-06T07:29:28.074957Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"indextokey_set = set(word_index.keys())\nprint(indextokey_set)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-06T07:29:28.078826Z","iopub.execute_input":"2022-09-06T07:29:28.079383Z","iopub.status.idle":"2022-09-06T07:29:28.090068Z","shell.execute_reply.started":"2022-09-06T07:29:28.079331Z","shell.execute_reply":"2022-09-06T07:29:28.088834Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"def avg_feature_vector_glove(sentence,embedding_matrix, num_features, index2word_set):\n    words = sentence.split()\n    feature_vec = np.zeros((num_features, ), dtype='float32')\n    n_words = 0\n    for word in words:\n            if word in index2word_set:\n                n_words += 1\n                feature_vec = np.add(feature_vec,embedding_matrix[word_index[word]]) \n            if (n_words > 0):\n                feature_vec = np.divide(feature_vec, n_words)\n    return feature_vec  ","metadata":{"execution":{"iopub.status.busy":"2022-09-06T07:29:28.093108Z","iopub.execute_input":"2022-09-06T07:29:28.093765Z","iopub.status.idle":"2022-09-06T07:29:28.103072Z","shell.execute_reply.started":"2022-09-06T07:29:28.093709Z","shell.execute_reply":"2022-09-06T07:29:28.101659Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"score_1 = []\nfor sentence in data['job_title']:\n    s1_afv = avg_feature_vector_glove(sentence,embedding_matrix,num_features=300, index2word_set=indextokey_set)     \n    score_1.append(s1_afv)\nprint(len(score_1))\n    ","metadata":{"execution":{"iopub.status.busy":"2022-09-06T07:29:28.104347Z","iopub.execute_input":"2022-09-06T07:29:28.104856Z","iopub.status.idle":"2022-09-06T07:29:28.123272Z","shell.execute_reply.started":"2022-09-06T07:29:28.104815Z","shell.execute_reply":"2022-09-06T07:29:28.122124Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"score_2 = []\nfor sentence in data['input']:\n    s1_afv = avg_feature_vector_glove(sentence,embedding_matrix, num_features=300, index2word_set=indextokey_set)     \n    score_2.append(s1_afv)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-06T07:29:28.124972Z","iopub.execute_input":"2022-09-06T07:29:28.125426Z","iopub.status.idle":"2022-09-06T07:29:28.138971Z","shell.execute_reply.started":"2022-09-06T07:29:28.125387Z","shell.execute_reply":"2022-09-06T07:29:28.137424Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"# the output is Cosine simlilarity with word2vec word embedding \nsim_glove = []\nfor i in range(len(score1)):\n    sim = 1 - spatial.distance.cosine(score_1[i],score_2[i])\n    sim_glove.append(sim)\nprint(sim_glove)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T07:29:28.141042Z","iopub.execute_input":"2022-09-06T07:29:28.141587Z","iopub.status.idle":"2022-09-06T07:29:28.157145Z","shell.execute_reply.started":"2022-09-06T07:29:28.141532Z","shell.execute_reply":"2022-09-06T07:29:28.156048Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"markdown","source":"# **Cosine Similarity**","metadata":{}},{"cell_type":"code","source":"# Cosine Similarity with word2vec, tfidf, and glove embedding \nresult_similarity = []\nfor i in range(len(sim_word2vec)):\n    score_mean = (sim_word2vec[i]+sim_tfidf[i]+sim_glove[i])/3\n    result_similarity.append(score_mean)\nprint(result_similarity)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T07:29:28.158197Z","iopub.execute_input":"2022-09-06T07:29:28.158655Z","iopub.status.idle":"2022-09-06T07:29:28.166917Z","shell.execute_reply.started":"2022-09-06T07:29:28.158600Z","shell.execute_reply":"2022-09-06T07:29:28.165834Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"data['result_similarity'] = result_similarity","metadata":{"execution":{"iopub.status.busy":"2022-09-06T07:29:28.168051Z","iopub.execute_input":"2022-09-06T07:29:28.168430Z","iopub.status.idle":"2022-09-06T07:29:28.181282Z","shell.execute_reply.started":"2022-09-06T07:29:28.168394Z","shell.execute_reply":"2022-09-06T07:29:28.179903Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"# cut the connection variable as 1,2,3,4, the larger the higher rank \ndata['connection'] = data['connection'].str.split('+').str[0]\ndata['connection'] = data['connection'].astype(int)\nlist = data['connection'].tolist()\nlist.sort()\n# print(list)\ndata['connection'] = pd.cut(data['connection'],[0,40,200,499,600],labels=[1,2,3,4]) \ndata['connection'] = data['connection'].astype('int')","metadata":{"execution":{"iopub.status.busy":"2022-09-06T07:29:28.186870Z","iopub.execute_input":"2022-09-06T07:29:28.187323Z","iopub.status.idle":"2022-09-06T07:29:28.204326Z","shell.execute_reply.started":"2022-09-06T07:29:28.187282Z","shell.execute_reply":"2022-09-06T07:29:28.203112Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"# one hot encoding location variable\ndum_key = pd.get_dummies(data['location'])\ndata = data.drop('location', 1)\ndata = pd.concat([data,dum_key],axis=1)\ndata.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T07:29:28.205736Z","iopub.execute_input":"2022-09-06T07:29:28.207083Z","iopub.status.idle":"2022-09-06T07:29:28.240563Z","shell.execute_reply.started":"2022-09-06T07:29:28.207037Z","shell.execute_reply":"2022-09-06T07:29:28.239303Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"data[data['result_similarity'].astype(float)>0.5] ='1'\ndata[data['result_similarity'].astype(float)<0.5] ='0'","metadata":{"execution":{"iopub.status.busy":"2022-09-06T07:37:19.140054Z","iopub.execute_input":"2022-09-06T07:37:19.140601Z","iopub.status.idle":"2022-09-06T07:37:19.151362Z","shell.execute_reply.started":"2022-09-06T07:37:19.140559Z","shell.execute_reply":"2022-09-06T07:37:19.149758Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"# set result_similarity column as label\ny = data['result_similarity']\nx = data.drop(['id','fit','job_title','input','result_similarity'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T07:37:20.224276Z","iopub.execute_input":"2022-09-06T07:37:20.225703Z","iopub.status.idle":"2022-09-06T07:37:20.233735Z","shell.execute_reply.started":"2022-09-06T07:37:20.225636Z","shell.execute_reply":"2022-09-06T07:37:20.232288Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split( x, y,test_size=0.7, random_state=26)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T07:37:21.286978Z","iopub.execute_input":"2022-09-06T07:37:21.287972Z","iopub.status.idle":"2022-09-06T07:37:21.297193Z","shell.execute_reply.started":"2022-09-06T07:37:21.287926Z","shell.execute_reply":"2022-09-06T07:37:21.295776Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nclassifier.fit(x_train, y_train)\n# Predicting the Test set results\ny_pred = classifier.predict(x_test)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T07:37:22.002219Z","iopub.execute_input":"2022-09-06T07:37:22.003309Z","iopub.status.idle":"2022-09-06T07:37:22.035923Z","shell.execute_reply.started":"2022-09-06T07:37:22.003261Z","shell.execute_reply":"2022-09-06T07:37:22.034658Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"y_pred ","metadata":{"execution":{"iopub.status.busy":"2022-09-06T07:50:14.967369Z","iopub.execute_input":"2022-09-06T07:50:14.967900Z","iopub.status.idle":"2022-09-06T07:50:14.977111Z","shell.execute_reply.started":"2022-09-06T07:50:14.967855Z","shell.execute_reply":"2022-09-06T07:50:14.975755Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}