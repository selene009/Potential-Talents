{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-09-07T06:39:30.608055Z","iopub.execute_input":"2022-09-07T06:39:30.609563Z","iopub.status.idle":"2022-09-07T06:39:30.622649Z","shell.execute_reply.started":"2022-09-07T06:39:30.609507Z","shell.execute_reply":"2022-09-07T06:39:30.621168Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('../input/potential-talents/potential-talents - Aspiring human resources - seeking human resources.csv')","metadata":{"execution":{"iopub.status.busy":"2022-09-07T06:39:31.106380Z","iopub.execute_input":"2022-09-07T06:39:31.108939Z","iopub.status.idle":"2022-09-07T06:39:31.118394Z","shell.execute_reply.started":"2022-09-07T06:39:31.108871Z","shell.execute_reply":"2022-09-07T06:39:31.117376Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"data['input'] = \"Aspiring human resources seeking human resources\"","metadata":{"execution":{"iopub.status.busy":"2022-09-07T06:39:31.412539Z","iopub.execute_input":"2022-09-07T06:39:31.413720Z","iopub.status.idle":"2022-09-07T06:39:31.420923Z","shell.execute_reply.started":"2022-09-07T06:39:31.413668Z","shell.execute_reply":"2022-09-07T06:39:31.419521Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"from nltk import word_tokenize, pos_tag\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\nimport re \nimport string \ndef removestopwords(text):\n    stop_words = set(stopwords.words('english'))\n#     print(stop_words)\n    text = re.sub(r'[^\\w\\s]', \" \", text)\n    text  = [word for word in text.split() if word not in stop_words]\n    \n    return text    \n\ndef lemmatization(text):\n    words = removestopwords(text)\n#     print(words)\n#     words = word_tokenize(words)\n#     word_tagged = pos_tag(words)\n#     print( word_tagged)\n    lemmatizer = WordNetLemmatizer()\n    text = \" \".join([lemmatizer.lemmatize(word) for word in words])\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-09-07T06:39:31.715613Z","iopub.execute_input":"2022-09-07T06:39:31.716325Z","iopub.status.idle":"2022-09-07T06:39:31.726064Z","shell.execute_reply.started":"2022-09-07T06:39:31.716276Z","shell.execute_reply":"2022-09-07T06:39:31.724992Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"import nltk\nnltk.download('omw-1.4')","metadata":{"execution":{"iopub.status.busy":"2022-09-07T06:39:31.941542Z","iopub.execute_input":"2022-09-07T06:39:31.941969Z","iopub.status.idle":"2022-09-07T06:39:31.950703Z","shell.execute_reply.started":"2022-09-07T06:39:31.941931Z","shell.execute_reply":"2022-09-07T06:39:31.949476Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"data['job_title'] = data['job_title'].apply(lambda x: lemmatization(x))\ndata['job_title'] ","metadata":{"execution":{"iopub.status.busy":"2022-09-07T06:39:32.306193Z","iopub.execute_input":"2022-09-07T06:39:32.306642Z","iopub.status.idle":"2022-09-07T06:39:32.340939Z","shell.execute_reply.started":"2022-09-07T06:39:32.306604Z","shell.execute_reply":"2022-09-07T06:39:32.339755Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"markdown","source":"# **word2vec**","metadata":{}},{"cell_type":"code","source":"import nltk\nimport gensim\nfrom gensim.models import Word2Vec\ntokens = data['job_title'].apply(lambda x: nltk.word_tokenize(x))\nw2v_model = Word2Vec(tokens,\n                     min_count=1,\n                     window=10,\n                     vector_size=250,\n                     alpha=0.03, \n                     min_alpha=0.0007,\n                     workers = 4,\n                     seed = 42)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T06:39:33.027478Z","iopub.execute_input":"2022-09-07T06:39:33.027911Z","iopub.status.idle":"2022-09-07T06:39:33.075327Z","shell.execute_reply.started":"2022-09-07T06:39:33.027874Z","shell.execute_reply":"2022-09-07T06:39:33.074236Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"from scipy import spatial\n    \nindextokey_set = set(w2v_model.wv.index_to_key)\ndef avg_feature_vector(sentence, w2v_model, num_features, index2word_set):\n    words = sentence.split()\n    feature_vec = np.zeros((num_features, ), dtype='float32')\n    n_words = 0\n    for word in words:\n            if word in index2word_set:\n                n_words += 1\n                feature_vec = np.add(feature_vec, w2v_model.wv[word])\n                \n            if (n_words > 0):\n                feature_vec = np.divide(feature_vec, n_words)\n    return feature_vec  ","metadata":{"execution":{"iopub.status.busy":"2022-09-07T06:39:33.442204Z","iopub.execute_input":"2022-09-07T06:39:33.443669Z","iopub.status.idle":"2022-09-07T06:39:33.452319Z","shell.execute_reply.started":"2022-09-07T06:39:33.443621Z","shell.execute_reply":"2022-09-07T06:39:33.451254Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"score1 = []\nfor sentence in data['job_title']:\n    s1_afv = avg_feature_vector(sentence, w2v_model, num_features=250, index2word_set=indextokey_set)     \n    score1.append(s1_afv)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T06:39:33.792354Z","iopub.execute_input":"2022-09-07T06:39:33.793128Z","iopub.status.idle":"2022-09-07T06:39:33.803737Z","shell.execute_reply.started":"2022-09-07T06:39:33.793087Z","shell.execute_reply":"2022-09-07T06:39:33.802286Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"score2 = []\nfor sentence in data['input']:\n    s2_afv = [avg_feature_vector(sentence, w2v_model, num_features=250, index2word_set=indextokey_set)]\n    score2.append(s2_afv)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T06:39:34.167963Z","iopub.execute_input":"2022-09-07T06:39:34.168402Z","iopub.status.idle":"2022-09-07T06:39:34.178264Z","shell.execute_reply.started":"2022-09-07T06:39:34.168364Z","shell.execute_reply":"2022-09-07T06:39:34.177007Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"# the output is Cosine simlilarity with word2vec word embedding \nsim_word2vec = []\nfor i in range(len(score1)):\n    sim = 1 - spatial.distance.cosine(score1[i],score2[i])\n    sim_word2vec.append(sim)\nprint(sim_word2vec)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T06:39:34.865298Z","iopub.execute_input":"2022-09-07T06:39:34.865756Z","iopub.status.idle":"2022-09-07T06:39:34.881991Z","shell.execute_reply.started":"2022-09-07T06:39:34.865720Z","shell.execute_reply":"2022-09-07T06:39:34.880516Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"markdown","source":"# **TFIDF**","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\ndef cosine_sim(text1, text2):\n    tfidf = vectorizer.fit_transform([text1, text2])\n    return ((tfidf * tfidf.T).A)[0,1]","metadata":{"execution":{"iopub.status.busy":"2022-09-07T06:39:35.316156Z","iopub.execute_input":"2022-09-07T06:39:35.316601Z","iopub.status.idle":"2022-09-07T06:39:35.324444Z","shell.execute_reply.started":"2022-09-07T06:39:35.316565Z","shell.execute_reply":"2022-09-07T06:39:35.322900Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"# the output is Cosine simlilarity with tfidf vectorizer \nsim_tfidf = []\nfor i in range(len(data['job_title'])):\n    score = cosine_sim(data['job_title'][i], data['input'][i])\n    sim_tfidf.append(score)\nprint(sim_tfidf)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T06:39:35.683815Z","iopub.execute_input":"2022-09-07T06:39:35.684224Z","iopub.status.idle":"2022-09-07T06:39:35.856669Z","shell.execute_reply.started":"2022-09-07T06:39:35.684192Z","shell.execute_reply":"2022-09-07T06:39:35.855508Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"markdown","source":"# **Glove**","metadata":{}},{"cell_type":"code","source":"# !wget http://nlp.stanford.edu/data/glove.6B.zip\n# !unzip glove.6B.zip","metadata":{"execution":{"iopub.status.busy":"2022-09-07T06:39:36.427208Z","iopub.execute_input":"2022-09-07T06:39:36.427659Z","iopub.status.idle":"2022-09-07T06:39:36.432383Z","shell.execute_reply.started":"2022-09-07T06:39:36.427622Z","shell.execute_reply":"2022-09-07T06:39:36.431504Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"GLOVE_EMB = '../input/glove6b300dtxt/glove.6B.300d.txt'\nEMBEDDING_DIM = 300","metadata":{"execution":{"iopub.status.busy":"2022-09-07T06:39:36.794501Z","iopub.execute_input":"2022-09-07T06:39:36.794959Z","iopub.status.idle":"2022-09-07T06:39:36.801183Z","shell.execute_reply.started":"2022-09-07T06:39:36.794922Z","shell.execute_reply":"2022-09-07T06:39:36.799686Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(data['job_title'])\n\nword_index = tokenizer.word_index\nvocab_size = len(tokenizer.word_index) + 1\nprint(\"Vocabulary Size :\", vocab_size)\nword_index.keys()","metadata":{"execution":{"iopub.status.busy":"2022-09-07T06:39:37.233810Z","iopub.execute_input":"2022-09-07T06:39:37.234231Z","iopub.status.idle":"2022-09-07T06:39:37.247114Z","shell.execute_reply.started":"2022-09-07T06:39:37.234198Z","shell.execute_reply":"2022-09-07T06:39:37.245794Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"embeddings_index = {}\n\nf = open(GLOVE_EMB)\nfor line in f:\n  values = line.split()\n  word = value = values[0]\n  coefs = np.asarray(values[1:], dtype='float32')\n  embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' %len(embeddings_index))","metadata":{"execution":{"iopub.status.busy":"2022-09-07T06:39:37.635060Z","iopub.execute_input":"2022-09-07T06:39:37.636321Z","iopub.status.idle":"2022-09-07T06:40:01.652659Z","shell.execute_reply.started":"2022-09-07T06:39:37.636280Z","shell.execute_reply":"2022-09-07T06:40:01.651219Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","metadata":{"execution":{"iopub.status.busy":"2022-09-07T06:40:01.656132Z","iopub.execute_input":"2022-09-07T06:40:01.657507Z","iopub.status.idle":"2022-09-07T06:40:01.665123Z","shell.execute_reply.started":"2022-09-07T06:40:01.657442Z","shell.execute_reply":"2022-09-07T06:40:01.663762Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"indextokey_set = set(word_index.keys())\nprint(indextokey_set)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-07T06:40:01.667252Z","iopub.execute_input":"2022-09-07T06:40:01.667793Z","iopub.status.idle":"2022-09-07T06:40:01.678002Z","shell.execute_reply.started":"2022-09-07T06:40:01.667755Z","shell.execute_reply":"2022-09-07T06:40:01.677012Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"def avg_feature_vector_glove(sentence,embedding_matrix, num_features, index2word_set):\n    words = sentence.split()\n    feature_vec = np.zeros((num_features, ), dtype='float32')\n    n_words = 0\n    for word in words:\n            if word in index2word_set:\n                n_words += 1\n                feature_vec = np.add(feature_vec,embedding_matrix[word_index[word]]) \n            if (n_words > 0):\n                feature_vec = np.divide(feature_vec, n_words)\n    return feature_vec  ","metadata":{"execution":{"iopub.status.busy":"2022-09-07T06:40:01.680682Z","iopub.execute_input":"2022-09-07T06:40:01.681078Z","iopub.status.idle":"2022-09-07T06:40:01.689721Z","shell.execute_reply.started":"2022-09-07T06:40:01.681044Z","shell.execute_reply":"2022-09-07T06:40:01.688807Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"score_1 = []\nfor sentence in data['job_title']:\n    s1_afv = avg_feature_vector_glove(sentence,embedding_matrix,num_features=300, index2word_set=indextokey_set)     \n    score_1.append(s1_afv)\nprint(len(score_1))\n    ","metadata":{"execution":{"iopub.status.busy":"2022-09-07T06:40:01.691340Z","iopub.execute_input":"2022-09-07T06:40:01.692034Z","iopub.status.idle":"2022-09-07T06:40:01.705302Z","shell.execute_reply.started":"2022-09-07T06:40:01.691988Z","shell.execute_reply":"2022-09-07T06:40:01.704211Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"score_2 = []\nfor sentence in data['input']:\n    s1_afv = avg_feature_vector_glove(sentence,embedding_matrix, num_features=300, index2word_set=indextokey_set)     \n    score_2.append(s1_afv)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-07T06:40:01.706781Z","iopub.execute_input":"2022-09-07T06:40:01.707403Z","iopub.status.idle":"2022-09-07T06:40:01.719331Z","shell.execute_reply.started":"2022-09-07T06:40:01.707367Z","shell.execute_reply":"2022-09-07T06:40:01.717944Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"# the output is Cosine simlilarity with word2vec word embedding \nsim_glove = []\nfor i in range(len(score1)):\n    sim = 1 - spatial.distance.cosine(score_1[i],score_2[i])\n    sim_glove.append(sim)\nprint(sim_glove)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T06:40:01.721202Z","iopub.execute_input":"2022-09-07T06:40:01.722160Z","iopub.status.idle":"2022-09-07T06:40:01.739930Z","shell.execute_reply.started":"2022-09-07T06:40:01.722120Z","shell.execute_reply":"2022-09-07T06:40:01.738925Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"markdown","source":"# **Cosine Similarity**","metadata":{}},{"cell_type":"code","source":"# Cosine Similarity with word2vec, tfidf, and glove embedding \nresult_similarity = []\nfor i in range(len(sim_word2vec)):\n    score_mean = (sim_word2vec[i]+sim_tfidf[i]+sim_glove[i])/3\n    result_similarity.append(score_mean)\nprint(result_similarity)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T06:40:01.741498Z","iopub.execute_input":"2022-09-07T06:40:01.742102Z","iopub.status.idle":"2022-09-07T06:40:01.748606Z","shell.execute_reply.started":"2022-09-07T06:40:01.742064Z","shell.execute_reply":"2022-09-07T06:40:01.747631Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"data['result_similarity'] = result_similarity","metadata":{"execution":{"iopub.status.busy":"2022-09-07T06:40:01.750188Z","iopub.execute_input":"2022-09-07T06:40:01.750884Z","iopub.status.idle":"2022-09-07T06:40:01.762998Z","shell.execute_reply.started":"2022-09-07T06:40:01.750846Z","shell.execute_reply":"2022-09-07T06:40:01.761709Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"# cut the connection variable as 1,2,3,4, the larger the higher rank \ndata['connection'] = data['connection'].str.split('+').str[0]\ndata['connection'] = data['connection'].astype(int)\nlist = data['connection'].tolist()\nlist.sort()\n# print(list)\ndata['connection'] = pd.cut(data['connection'],[0,40,200,499,600],labels=[1,2,3,4]) \ndata['connection'] = data['connection'].astype('int')","metadata":{"execution":{"iopub.status.busy":"2022-09-07T06:40:01.766398Z","iopub.execute_input":"2022-09-07T06:40:01.767025Z","iopub.status.idle":"2022-09-07T06:40:01.782302Z","shell.execute_reply.started":"2022-09-07T06:40:01.766989Z","shell.execute_reply":"2022-09-07T06:40:01.780906Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"# one hot encoding location variable\ndum_key = pd.get_dummies(data['location'])\ndata = data.drop('location', 1)\ndata = pd.concat([data,dum_key],axis=1)\ndata.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T06:40:01.783840Z","iopub.execute_input":"2022-09-07T06:40:01.784676Z","iopub.status.idle":"2022-09-07T06:40:01.815273Z","shell.execute_reply.started":"2022-09-07T06:40:01.784637Z","shell.execute_reply":"2022-09-07T06:40:01.813957Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"# data[data['result_similarity'].astype(float)>0.5] ='1'\n# data[data['result_similarity'].astype(float)<0.5] ='0'","metadata":{"execution":{"iopub.status.busy":"2022-09-07T06:40:01.817615Z","iopub.execute_input":"2022-09-07T06:40:01.819809Z","iopub.status.idle":"2022-09-07T06:40:01.825539Z","shell.execute_reply.started":"2022-09-07T06:40:01.819752Z","shell.execute_reply":"2022-09-07T06:40:01.823941Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"\n# set result_similarity column as label\ny = data['result_similarity']\nx = data.drop(['id','fit','job_title','input','result_similarity'], axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-07T06:40:01.827719Z","iopub.execute_input":"2022-09-07T06:40:01.828243Z","iopub.status.idle":"2022-09-07T06:40:01.841286Z","shell.execute_reply.started":"2022-09-07T06:40:01.828198Z","shell.execute_reply":"2022-09-07T06:40:01.839921Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"y","metadata":{"execution":{"iopub.status.busy":"2022-09-07T06:43:19.496860Z","iopub.execute_input":"2022-09-07T06:43:19.497278Z","iopub.status.idle":"2022-09-07T06:43:19.507321Z","shell.execute_reply.started":"2022-09-07T06:43:19.497244Z","shell.execute_reply":"2022-09-07T06:43:19.505957Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split( x, y,test_size=0.7, random_state=26)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-09-07T06:43:20.722362Z","iopub.execute_input":"2022-09-07T06:43:20.722825Z","iopub.status.idle":"2022-09-07T06:43:20.730458Z","shell.execute_reply.started":"2022-09-07T06:43:20.722786Z","shell.execute_reply":"2022-09-07T06:43:20.728872Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nclassifier.fit(x_train, y_train)\n# Predicting the Test set results\ny_pred = classifier.predict(x_test)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-09-07T06:43:21.075454Z","iopub.execute_input":"2022-09-07T06:43:21.076516Z","iopub.status.idle":"2022-09-07T06:43:21.083448Z","shell.execute_reply.started":"2022-09-07T06:43:21.076473Z","shell.execute_reply":"2022-09-07T06:43:21.082297Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"import random\ndef Starcandidate(n,similarity_score):\n    # as do not know the standard of starring candidate, suppose I random select some candidate\n    # n is the number of candidate who you want to star \n    candidate_list = similarity_score.tolist()\n    star_list = random.sample(candidate_list,n)\n    \n    # delete star_list from candidate_list\n    reminding_list = []\n    for item in candidate_list:\n        if item not in star_list:\n            reminding_list.append(item)\n#     print(reminding_list)\n            \n    # label star_list as 1\n    star_list_label = []\n    for i in range(len(star_list)):\n        star_list[i] = 1\n        star_list_label.append(star_list[i])\n#     print(star_list_label)\n\n   # label the reminding list. If a similarity score of >=0.5 as 1 and remaining with similarity score <0.5 as 0\n    reminding_list_label =[]\n    for i in range(len(reminding_list)):\n        if reminding_list[i] >= 0.5:\n            reminding_list[i] = 1\n            reminding_list_label.append(reminding_list[i])\n        elif reminding_list[i] <= 0.5:\n            reminding_list[i] = 0\n            reminding_list_label.append(reminding_list[i])\n        else: None\n    return  reminding_list_label\n             \n        \n        \n    \n        ","metadata":{"execution":{"iopub.status.busy":"2022-09-07T06:52:49.927937Z","iopub.execute_input":"2022-09-07T06:52:49.928457Z","iopub.status.idle":"2022-09-07T06:52:49.939610Z","shell.execute_reply.started":"2022-09-07T06:52:49.928396Z","shell.execute_reply":"2022-09-07T06:52:49.938136Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"Starcandidate(5,y)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T06:53:00.275213Z","iopub.execute_input":"2022-09-07T06:53:00.275674Z","iopub.status.idle":"2022-09-07T06:53:00.284728Z","shell.execute_reply.started":"2022-09-07T06:53:00.275636Z","shell.execute_reply":"2022-09-07T06:53:00.283374Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}